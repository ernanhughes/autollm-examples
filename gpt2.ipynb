{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"data.txt\"\n",
    "text = open(data_dir, 'r').read() # load all the data as simple string\n",
    "\n",
    "# Get all unique characters in the text as vocabulary\n",
    "chars = list(set(text))\n",
    "vocab_size = len(chars)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the character level tokenizer\n",
    "chr_to_idx = {c:i for i, c in enumerate(chars)}\n",
    "idx_to_chr = {i:c for i, c in enumerate(chars)}\n",
    "\n",
    "def encode(input_text: str) -> list[int]:\n",
    "    return [chr_to_idx[t] for t in input_text]\n",
    "\n",
    "def decode(input_tokens: list[int]) -> str:\n",
    "    return \"\".join([idx_to_chr[i] for i in input_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23, 13, 40, 40, 10, 69, 58, 10, 38, 40, 6]\n",
      "this is a test\n"
     ]
    }
   ],
   "source": [
    "print(encode(\"Hello World\"))\n",
    "print(decode(encode(\"this is a test\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: [72, 101, 108, 108, 111, 44, 32, 87, 111, 114, 108, 100, 33]\n",
      "Tensor Text: tensor([ 72, 101, 108, 108, 111,  44,  32,  87, 111, 114, 108, 100,  33])\n",
      "Using device: cuda\n",
      "Tensor on GPU: tensor([ 72, 101, 108, 108, 111,  44,  32,  87, 111, 114, 108, 100,  33],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Simple character-level tokenizer\n",
    "def char_tokenizer(text):\n",
    "    return [ord(c) for c in text]\n",
    "\n",
    "# Example text\n",
    "text = \"Hello, World!\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokenized_text = char_tokenizer(text)\n",
    "print(\"Tokenized Text:\", tokenized_text)\n",
    "\n",
    "# Convert tokenized text to tensor\n",
    "tensor_text = torch.tensor(tokenized_text, dtype=torch.long)\n",
    "print(\"Tensor Text:\", tensor_text)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Move tensor to GPU\n",
    "tensor_text = tensor_text.to(device)\n",
    "print(\"Tensor on GPU:\", tensor_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "# use cpu or gpu based on your system\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "# convert our text data into tokenized tensor\n",
    "dt = torch.tensor(encode(text)[:50])\n",
    "len(dt)\n",
    "type(dt)\n",
    "\n",
    "\n",
    "#dt.to(device)\n",
    "\n",
    "#data = torch.tensor(encode(text))\n",
    "#data = data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 16  # training batch size\n",
    "eval_batch_size = 8  # evaluation batch size\n",
    "context_length = 256  # number of tokens processed in a single batch\n",
    "train_split = 0.8  # percentage of data to use from total data for training\n",
    "\n",
    "# split data into trian and eval\n",
    "n_data = len(data)\n",
    "train_data = data[:int(n_data * train_split)]\n",
    "eval_data = data[int(n_data * train_split):]\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, tokens, batch_size, context_length) -> None:\n",
    "        self.tokens = tokens\n",
    "        self.batch_size = batch_size\n",
    "        self.context_length = context_length\n",
    "\n",
    "        self.current_position = 0\n",
    "\n",
    "    def get_batch(self) -> torch.tensor:\n",
    "        b, c = self.batch_size, self.context_length\n",
    "\n",
    "        start_pos = self.current_position\n",
    "        end_pos = self.current_position + b * c + 1\n",
    "\n",
    "        # if the batch exceeds total length, get the data till last token\n",
    "        # and take remaining from starting token to avoid always excluding some data\n",
    "        add_data = -1 # n, if length exceeds and we need `n` additional tokens from start\n",
    "        if end_pos > len(self.tokens):\n",
    "            add_data = end_pos - len(self.tokens) - 1\n",
    "            end_pos = len(self.tokens) - 1\n",
    "\n",
    "        d = self.tokens[start_pos:end_pos]\n",
    "        if add_data != -1:\n",
    "            d = torch.cat([d, self.tokens[:add_data]])\n",
    "        x = (d[:-1]).view(b, c)  # inputs\n",
    "        y = (d[1:]).view(b, c)  # targets\n",
    "\n",
    "        self.current_position += b * c # set the next position\n",
    "        return x, y\n",
    "\n",
    "train_loader = DataLoader(train_data, train_batch_size, context_length)\n",
    "eval_loader = DataLoader(eval_data, eval_batch_size, context_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 256]) torch.Size([16, 256])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "163569"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = train_loader.get_batch()\n",
    "print(xb.shape, yb.shape)\n",
    "len(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 151\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [tokenizer\u001b[38;5;241m.\u001b[39mdecode(out\u001b[38;5;241m.\u001b[39mtolist()) \u001b[38;5;28;01mfor\u001b[39;00m out \u001b[38;5;129;01min\u001b[39;00m output]\n\u001b[0;32m    150\u001b[0m m \u001b[38;5;241m=\u001b[39m GPT(vocab_size\u001b[38;5;241m=\u001b[39mvocab_size, d_model\u001b[38;5;241m=\u001b[39md_model, n_heads\u001b[38;5;241m=\u001b[39mn_heads, n_layers\u001b[38;5;241m=\u001b[39mn_layers)\n\u001b[1;32m--> 151\u001b[0m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m m \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcompile(m)\n",
      "File \u001b[1;32mc:\\Users\\ernan\\anaconda3\\envs\\pneumonia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ernan\\anaconda3\\envs\\pneumonia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ernan\\anaconda3\\envs\\pneumonia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ernan\\anaconda3\\envs\\pneumonia\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# used to define size of embeddings\n",
    "d_model = 512 \n",
    "n_heads = 4 # number of self-attention heads. should be divisible with d_model\n",
    "n_layers = 1 # number of gpt blocks/layers\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, n_heads: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "\n",
    "        assert (n_heads * self.head_dim == d_model)\n",
    "\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor):\n",
    "        B, seq_length, d_model = inputs.shape\n",
    "        \n",
    "        # Project the input embeddings into Q, K, and V\n",
    "        Q = self.query(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = self.key(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = self.value(inputs).view(B, seq_length, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Compute attention scores\n",
    "        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        # Apply mask to prevent attention to future tokens\n",
    "        mask = torch.triu(torch.ones(seq_length, seq_length), diagonal=1).bool().to(inputs.device)\n",
    "        attention_scores = attention_scores.masked_fill(mask, float('-inf'))\n",
    "        \n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        # Compute the weighted sum of the values\n",
    "        attention_output = torch.matmul(self.dropout(attention_weights), V)\n",
    "\n",
    "        # Concatenate heads and put them back to the original shape\n",
    "        attention_output = attention_output.permute(0, 2, 1, 3).contiguous()\n",
    "        attention_output = attention_output.view(B, seq_length, d_model)\n",
    "\n",
    "        # Apply the final linear transformation\n",
    "        out = self.fc_out(attention_output)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, context_length, d_model) -> None:\n",
    "        super().__init__()\n",
    "        # Create a matrix of shape (context_length, d_model) to store the positional encodings\n",
    "        pe = torch.zeros(context_length, d_model)\n",
    "        \n",
    "        # Create a vector with positions [0, 1, 2, ..., context_length-1] of shape (context_length, 1)\n",
    "        position = torch.arange(0, context_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Create a vector with the divisor terms based on the dimension\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # Compute the positional encodings using sine and cosine functions\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # Shape: (1, context_length, d_model)\n",
    "        \n",
    "        # Register pe as a buffer, so it is not considered a parameter but is part of the module's state\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Add the positional encodings to the input embeddings\n",
    "        return x + self.pe[:,:x.size(1), :] \n",
    "    \n",
    "\n",
    "class GPTBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fcn = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, logits):\n",
    "        att_logits = self.att(logits)\n",
    "        adn_logits = self.ln1(logits + att_logits)\n",
    "        logits = self.dropout(adn_logits)\n",
    "        logits = self.fcn(logits)\n",
    "        logits = self.ln2(logits + adn_logits)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.wte = nn.Embedding(vocab_size, d_model) # word token embeddings\n",
    "        self.wpe = PositionalEncoding(context_length, d_model) # word position encodings\n",
    "        self.blocks = nn.ModuleList([GPTBlock(d_model, n_heads) for _ in  range(n_layers)])\n",
    "        self.linear1 = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        self.wte.weight = self.linear1.weight\n",
    "\n",
    "    def forward(self, inputs, targets = None):\n",
    "        logits = self.wte(inputs) # dim -> batch_size, sequence_length, d_model\n",
    "        logits = self.wpe(logits)\n",
    "        for block in self.blocks:\n",
    "            logits = block(logits)\n",
    "        logits = self.linear1(logits)\n",
    "        loss = None\n",
    "        if targets != None:\n",
    "            batch_size, sequence_length, d_model = logits.shape\n",
    "            # to calculate loss for all token embeddings in a batch\n",
    "            # kind of a requirement for cross_entropy\n",
    "            logits = logits.view(batch_size * sequence_length, d_model)\n",
    "            targets = targets.view(batch_size * sequence_length)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, inputs, max_new_tokens):\n",
    "        # this will store the model outputs along with the initial input sequence\n",
    "        # make a copy so that it doesn't interfare with model \n",
    "        output = inputs.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            current_seq_length = inputs.size(1)\n",
    "            # Truncate inputs if it exceeds context_length\n",
    "            if current_seq_length > context_length:\n",
    "                inputs = inputs[:, -context_length:]\n",
    "            # we only pass targets on training to calculate loss\n",
    "            logits, _ = self(inputs)  \n",
    "            # for all the batches, get the embeds for last predicted sequence\n",
    "            logits = logits[:, -1, :] \n",
    "            probs = F.softmax(logits, dim=1)            \n",
    "            # get the probable token based on the input probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) \n",
    "            \n",
    "            inputs = torch.cat([inputs, idx_next], dim=1)\n",
    "            output = torch.cat([output, idx_next], dim=1)\n",
    "        return [tokenizer.decode(out.tolist()) for out in output]\n",
    "\n",
    "m = GPT(vocab_size=vocab_size, d_model=d_model, n_heads=n_heads, n_layers=n_layers)\n",
    "m.to(device)\n",
    "m = torch.compile(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OptimizedModule(\n",
      "  (_orig_mod): GPT(\n",
      "    (wte): Embedding(89, 512)\n",
      "    (wpe): PositionalEncoding()\n",
      "    (blocks): ModuleList(\n",
      "      (0): GPTBlock(\n",
      "        (att): MultiHeadAttention(\n",
      "          (query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (key): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (value): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.2, inplace=False)\n",
      "        )\n",
      "        (ln1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (ln2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.2, inplace=False)\n",
      "        (fcn): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (linear1): Linear(in_features=512, out_features=89, bias=True)\n",
      "  )\n",
      ")\n",
      "Total Parameters: 3M\n"
     ]
    }
   ],
   "source": [
    "print(m)\n",
    "print(f\"Total Parameters: {round(sum(p.numel() for p in m.parameters() if p.requires_grad) / 1_000_000)}M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "vocab_size = tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"CUDA_LAUNCH_BLOCKING\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# saying to torch that do not store gradients for whatever we do below\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLove \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(m\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;28minput\u001b[39m, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "m.eval()\n",
    "\n",
    "# saying to torch that do not store gradients for whatever we do below\n",
    "with torch.no_grad():\n",
    "    input = torch.tensor(tokenizer.encode(\"Love \"), device=device).unsqueeze(0)\n",
    "    print(m.generate(input, max_new_tokens=1000)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pneumonia",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
