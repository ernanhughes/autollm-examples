{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://api.gradio.app/gradio-messaging/en \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "from autollm import AutoQueryEngine\n",
    "from autollm.utils.document_reading import read_github_repo_as_documents, read_files_as_documents\n",
    "import os\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-27 15:10:00,774 - autollm - INFO - Cloning github repo https://github.com/cpacker/MemGPT.git into temporary directory autollm\\temp..\n",
      "2024-06-27 15:10:03,101 - autollm - INFO - Reading files from autollm\\temp\\docs..\n",
      "Loading files: 100%|██████████| 30/30 [00:00<00:00, 360.25file/s]\n",
      "2024-06-27 15:10:03,192 - autollm - INFO - Found 30 'document(s)'.\n",
      "2024-06-27 15:10:03,192 - autollm - INFO - Operations complete, deleting temporary directory autollm\\temp..\n"
     ]
    }
   ],
   "source": [
    "git_repo_url = \"https://github.com/cpacker/MemGPT.git\"\n",
    "relative_folder_path = \"docs\"   # relative path from the repo root to the folder containing documents\n",
    "required_exts = [\".md\"]    # optional, only read files with these extensions\n",
    "\n",
    "documents = read_github_repo_as_documents(git_repo_url=git_repo_url, relative_folder_path=relative_folder_path, required_exts=required_exts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'AutoQueryEngine' has no attribute 'create_query_engine'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     23\u001b[0m service_context_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: chunk_size}\n\u001b[0;32m     24\u001b[0m query_engine_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_top_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: similarity_top_k}\n\u001b[1;32m---> 25\u001b[0m query_engine_new \u001b[38;5;241m=\u001b[39m \u001b[43mAutoQueryEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_query_engine\u001b[49m(documents\u001b[38;5;241m=\u001b[39mdocuments, system_prompt\u001b[38;5;241m=\u001b[39msystem_prompt, query_wrapper_prompt\u001b[38;5;241m=\u001b[39mquery_wrapper_prompt, enable_cost_calculator\u001b[38;5;241m=\u001b[39menable_cost_calculator, llm_params\u001b[38;5;241m=\u001b[39mllm_params, vector_store_params \u001b[38;5;241m=\u001b[39m vector_store_params, service_context_params\u001b[38;5;241m=\u001b[39mservice_context_params, query_engine_params\u001b[38;5;241m=\u001b[39mquery_engine_params)\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'AutoQueryEngine' has no attribute 'create_query_engine'"
     ]
    }
   ],
   "source": [
    "system_prompt = \"You are an AI assitant that helps users find the most relevant and accurate answers to their questions.\"\n",
    "\n",
    "query_wrapper_prompt = \"\"\"\n",
    "The document information is below.\n",
    "-------------------------------\n",
    "{context_str}\n",
    "-------------------------------\n",
    "Using the document information and mostly relying on it,\n",
    "answer the query.\n",
    "Query: {query_str}\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "enable_cost_calculator = True\n",
    "model = \"gpt-3.5-turbo\"\n",
    "vector_store_type = \"LanceDBVectorStore\"\n",
    "uri= \"tmp/lancedb\"\n",
    "table_name = \"vectors\"\n",
    "chunk_size = 1024\n",
    "similarity_top_k = 5\n",
    "llm_params = {\"model\": model}\n",
    "vector_store_params = {\"vector_store_type\": vector_store_type, \"uri\":uri, \"table_name\":table_name}\n",
    "service_context_params = {\"chunk_size\": chunk_size}\n",
    "query_engine_params = {\"similarity_top_k\": similarity_top_k}\n",
    "query_engine_new = AutoQueryEngine.create_query_engine(documents=documents, system_prompt=system_prompt, query_wrapper_prompt=query_wrapper_prompt, enable_cost_calculator=enable_cost_calculator, llm_params=llm_params, vector_store_params = vector_store_params, service_context_params=service_context_params, query_engine_params=query_engine_params)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autollm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
